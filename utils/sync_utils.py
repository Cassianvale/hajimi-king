import json
import threading
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional

import requests

from common.Logger import logger
from common.config import Config
from utils.file_manager import file_manager, checkpoint


class SyncUtils:
    """åŒæ­¥å·¥å…·ç±»ï¼Œè´Ÿè´£å¼‚æ­¥å‘é€keysåˆ°å¤–éƒ¨åº”ç”¨"""

    def __init__(self):
        """åˆå§‹åŒ–åŒæ­¥å·¥å…·"""
        # Gemini Balancer é…ç½®
        self.balancer_url = Config.GEMINI_BALANCER_URL.rstrip('/') if Config.GEMINI_BALANCER_URL else ""
        self.balancer_auth = Config.GEMINI_BALANCER_AUTH
        self.balancer_sync_enabled = Config.parse_bool(Config.GEMINI_BALANCER_SYNC_ENABLED)
        self.balancer_enabled = bool(self.balancer_url and self.balancer_auth and self.balancer_sync_enabled)

        # GPT Load Balancer é…ç½®
        self.gpt_load_url = Config.GPT_LOAD_URL.rstrip('/') if Config.GPT_LOAD_URL else ""
        self.gpt_load_auth = Config.GPT_LOAD_AUTH
        # è§£æå¤šä¸ªgroup names (é€—å·åˆ†éš”)
        self.gpt_load_group_names = [name.strip() for name in Config.GPT_LOAD_GROUP_NAME.split(',') if name.strip()] if Config.GPT_LOAD_GROUP_NAME else []
        self.gpt_load_sync_enabled = Config.parse_bool(Config.GPT_LOAD_SYNC_ENABLED)
        self.gpt_load_enabled = bool(self.gpt_load_url and self.gpt_load_auth and self.gpt_load_group_names and self.gpt_load_sync_enabled)

        # åˆ›å»ºçº¿ç¨‹æ± ç”¨äºå¼‚æ­¥æ‰§è¡Œ
        self.executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="SyncUtils")
        self.saving_checkpoint = False

        # å‘¨æœŸæ€§å‘é€æ§åˆ¶
        self.batch_interval = 60
        self.batch_timer = None
        self.shutdown_flag = False

        # GPT Load Balancer group ID ç¼“å­˜ (15åˆ†é’Ÿç¼“å­˜)
        self.group_id_cache: Dict[str, int] = {}
        self.group_id_cache_time: Dict[str, float] = {}
        self.group_id_cache_ttl = 15 * 60  # 15åˆ†é’Ÿ

        if not self.balancer_enabled:
            logger.warning("ğŸš« Gemini Balancer åŒæ­¥å·²ç¦ç”¨ - æœªé…ç½® URL æˆ– AUTH")
        else:
            logger.info(f"ğŸ”— Gemini Balancer å·²å¯ç”¨ - URL: {self.balancer_url}")

        if not self.gpt_load_enabled:
            logger.warning("ğŸš« GPT Load Balancer åŒæ­¥å·²ç¦ç”¨ - æœªé…ç½® URLã€AUTHã€GROUP_NAME æˆ–åŒæ­¥å·²ç¦ç”¨")
        else:
            logger.info(f"ğŸ”— GPT Load Balancer å·²å¯ç”¨ - URL: {self.gpt_load_url}, ç»„: {', '.join(self.gpt_load_group_names)}")

        # å¯åŠ¨å‘¨æœŸæ€§å‘é€çº¿ç¨‹
        self._start_batch_sender()

    def add_keys_to_queue(self, keys: List[str]):
        """
        å°†keysåŒæ—¶æ·»åŠ åˆ°balancerå’ŒGPT loadçš„å‘é€é˜Ÿåˆ—
        
        Args:
            keys: API keysåˆ—è¡¨
        """
        if not keys:
            return

        # Acquire lock for checkpoint saving
        while self.saving_checkpoint:
            logger.info(f"ğŸ“¥ æ£€æŸ¥ç‚¹æ­£åœ¨ä¿å­˜ä¸­ï¼Œåœ¨å°† {len(keys)} ä¸ªå¯†é’¥æ·»åŠ åˆ°é˜Ÿåˆ—å‰ç­‰å¾…...")
            time.sleep(0.5)  # Small delay to prevent busy-waiting

        self.saving_checkpoint = True  # Acquire the lock
        try:

            # Gemini Balancer
            if self.balancer_enabled:
                initial_balancer_count = len(checkpoint.wait_send_balancer)
                checkpoint.wait_send_balancer.update(keys)
                new_balancer_count = len(checkpoint.wait_send_balancer)
                added_balancer_count = new_balancer_count - initial_balancer_count
                logger.info(f"ğŸ“¥ å·²å°† {added_balancer_count} ä¸ªå¯†é’¥æ·»åŠ åˆ° gemini balancer é˜Ÿåˆ— (æ€»è®¡: {new_balancer_count})")
            else:
                logger.info(f"ğŸš« Gemini Balancer å·²ç¦ç”¨ï¼Œè·³è¿‡ {len(keys)} ä¸ªå¯†é’¥çš„ gemini balancer é˜Ÿåˆ—")

            # GPT Load Balancer
            if self.gpt_load_enabled:
                initial_gpt_count = len(checkpoint.wait_send_gpt_load)
                checkpoint.wait_send_gpt_load.update(keys)
                new_gpt_count = len(checkpoint.wait_send_gpt_load)
                added_gpt_count = new_gpt_count - initial_gpt_count
                logger.info(f"ğŸ“¥ å·²å°† {added_gpt_count} ä¸ªå¯†é’¥æ·»åŠ åˆ° GPT load balancer é˜Ÿåˆ— (æ€»è®¡: {new_gpt_count})")
            else:
                logger.info(f"ğŸš« GPT Load Balancer å·²ç¦ç”¨ï¼Œè·³è¿‡ {len(keys)} ä¸ªå¯†é’¥çš„ GPT load balancer é˜Ÿåˆ—")

            file_manager.save_checkpoint(checkpoint)
        finally:
            self.saving_checkpoint = False  # Release the lock

    def _send_balancer_worker(self, keys: List[str]) -> str:
        """
        å®é™…æ‰§è¡Œå‘é€åˆ°balancerçš„å·¥ä½œå‡½æ•°ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
        
        Args:
            keys: API keysåˆ—è¡¨
            
        Returns:
            str: "ok" if success, otherwise an error code string.
        """
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨å‘é€ {len(keys)} ä¸ªå¯†é’¥åˆ° balancer...")

            # 1. è·å–å½“å‰é…ç½®
            config_url = f"{self.balancer_url}/api/config"
            headers = {
                'Cookie': f'auth_token={self.balancer_auth}',
                'User-Agent': 'HajimiKing/1.0'
            }

            logger.info(f"ğŸ“¥ æ­£åœ¨ä»ä»¥ä¸‹ä½ç½®è·å–å½“å‰é…ç½®: {config_url}")

            # è·å–å½“å‰é…ç½®
            response = requests.get(config_url, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"è·å–é…ç½®å¤±è´¥: HTTP {response.status_code} - {response.text}")
                return "get_config_failed_not_200"

            # è§£æé…ç½®
            config_data = response.json()

            # 2. è·å–å½“å‰çš„API_KEYSæ•°ç»„
            current_api_keys = config_data.get('API_KEYS', [])

            # 3. åˆå¹¶æ–°keysï¼ˆå»é‡ï¼‰
            existing_keys_set = set(current_api_keys)
            new_add_keys_set = set()
            for key in keys:
                if key not in existing_keys_set:
                    existing_keys_set.add(key)
                    new_add_keys_set.add(key)

            if len(new_add_keys_set) == 0:
                logger.info(f"â„¹ï¸ æ‰€æœ‰ {len(keys)} ä¸ªå¯†é’¥å·²å­˜åœ¨äº balancer ä¸­")
                # ä¸éœ€è¦è®°å½•å‘é€ç»“æœï¼Œå› ä¸ºæ²¡æœ‰å®é™…å‘é€æ–°å¯†é’¥
                return "ok"

            # 4. æ›´æ–°é…ç½®ä¸­çš„API_KEYS
            config_data['API_KEYS'] = list(existing_keys_set)

            logger.info(f"ğŸ“ æ­£åœ¨ä½¿ç”¨ {len(new_add_keys_set)} ä¸ªæ–°å¯†é’¥æ›´æ–° gemini balancer é…ç½®...")

            # 5. å‘é€æ›´æ–°åçš„é…ç½®åˆ°æœåŠ¡å™¨
            update_headers = headers.copy()
            update_headers['Content-Type'] = 'application/json'

            update_response = requests.put(
                config_url,
                headers=update_headers,
                json=config_data,
                timeout=60
            )

            if update_response.status_code != 200:
                logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: HTTP {update_response.status_code} - {update_response.text}")
                return "update_config_failed_not_200"

            # 6. éªŒè¯æ˜¯å¦æ·»åŠ æˆåŠŸ
            updated_config = update_response.json()
            updated_api_keys = updated_config.get('API_KEYS', [])
            updated_keys_set = set(updated_api_keys)

            failed_to_add = [key for key in new_add_keys_set if key not in updated_keys_set]

            if failed_to_add:
                logger.error(f"âŒ æ·»åŠ  {len(failed_to_add)} ä¸ªå¯†é’¥å¤±è´¥: {[key[:10] + '...' for key in failed_to_add]}")
                # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - éƒ¨åˆ†æˆåŠŸçš„æƒ…å†µ
                send_result = {}
                keys_to_log = []
                for key in new_add_keys_set:  # åªè®°å½•å°è¯•æ–°å¢çš„å¯†é’¥
                    if key in failed_to_add:
                        send_result[key] = "update_failed"
                        keys_to_log.append(key)
                    else:
                        send_result[key] = "ok"
                        keys_to_log.append(key)
                if keys_to_log:  # åªæœ‰å½“æœ‰éœ€è¦è®°å½•çš„å¯†é’¥æ—¶æ‰è®°å½•
                    file_manager.save_keys_send_result(keys_to_log, send_result)
                return "update_failed"


            logger.info(f"âœ… æ‰€æœ‰ {len(new_add_keys_set)} ä¸ªæ–°å¯†é’¥å·²æˆåŠŸæ·»åŠ åˆ° balancer.")
            
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - åªè®°å½•å®é™…æ–°å¢çš„å¯†é’¥
            send_result = {key: "ok" for key in new_add_keys_set}
            if send_result:  # åªæœ‰å½“æœ‰æ–°å¢å¯†é’¥æ—¶æ‰è®°å½•
                file_manager.save_keys_send_result(list(new_add_keys_set), send_result)
            
            return "ok"

        except requests.exceptions.Timeout:
            logger.error("âŒ è¿æ¥ balancer æ—¶è¯·æ±‚è¶…æ—¶")
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥
            send_result = {key: "timeout" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "timeout"
        except requests.exceptions.ConnectionError:
            logger.error("âŒ è¿æ¥ balancer å¤±è´¥")
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥
            send_result = {key: "connection_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "connection_error"
        except json.JSONDecodeError as e:
            logger.error(f"âŒ balancer è¿”å›çš„ JSON æ— æ•ˆ: {str(e)}")
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥
            send_result = {key: "json_decode_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "json_decode_error"
        except Exception as e:
            logger.error(f"âŒ å‘é€å¯†é’¥åˆ° balancer å¤±è´¥: {str(e)}")
            traceback.print_exc()
            # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥
            send_result = {key: "exception" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "exception"

    def _get_gpt_load_group_id(self, group_name: str) -> Optional[int]:
        """
        è·å–GPT Load Balancer group IDï¼Œå¸¦ç¼“å­˜åŠŸèƒ½
        
        Args:
            group_name: ç»„å
            
        Returns:
            Optional[int]: ç»„IDï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if (group_name in self.group_id_cache and
            group_name in self.group_id_cache_time and
            current_time - self.group_id_cache_time[group_name] < self.group_id_cache_ttl):
            logger.info(f"ğŸ“‹ ä½¿ç”¨ '{group_name}' çš„ç¼“å­˜ç»„ ID: {self.group_id_cache[group_name]}")
            return self.group_id_cache[group_name]
        
        # ç¼“å­˜è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è·å–
        try:
            groups_url = f"{self.gpt_load_url}/api/groups"
            headers = {
                'Authorization': f'Bearer {self.gpt_load_auth}',
                'User-Agent': 'HajimiKing/1.0'
            }

            logger.info(f"ğŸ“¥ æ­£åœ¨ä»ä»¥ä¸‹ä½ç½®è·å–ç»„ä¿¡æ¯: {groups_url}")

            response = requests.get(groups_url, headers=headers, timeout=30)

            if response.status_code != 200:
                logger.error(f"è·å–ç»„ä¿¡æ¯å¤±è´¥: HTTP {response.status_code} - {response.text}")
                return None

            groups_data = response.json()
            
            if groups_data.get('code') != 0:
                logger.error(f"ç»„ API è¿”å›é”™è¯¯: {groups_data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return None

            # æŸ¥æ‰¾æŒ‡å®šgroupçš„ID
            groups_list = groups_data.get('data', [])
            for group in groups_list:
                if group.get('name') == group_name:
                    group_id = group.get('id')
                    # æ›´æ–°ç¼“å­˜
                    self.group_id_cache[group_name] = group_id
                    self.group_id_cache_time[group_name] = current_time
                    logger.info(f"âœ… æ‰¾åˆ°å¹¶ç¼“å­˜äº†ç»„ '{group_name}'ï¼ŒID: {group_id}")
                    return group_id

            logger.error(f"åœ¨ç»„åˆ—è¡¨ä¸­æœªæ‰¾åˆ°ç»„ '{group_name}'")
            return None

        except Exception as e:
            logger.error(f"âŒ è·å– '{group_name}' çš„ç»„ ID å¤±è´¥: {str(e)}")
            return None

    def _send_gpt_load_worker(self, keys: List[str]) -> str:
        """
        å®é™…æ‰§è¡Œå‘é€åˆ°GPT load balancerçš„å·¥ä½œå‡½æ•°ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
        
        Args:
            keys: API keysåˆ—è¡¨
            
        Returns:
            str: "ok" if success, otherwise an error code string.
        """
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨å‘é€ {len(keys)} ä¸ªå¯†é’¥åˆ° GPT load balancerï¼Œå…± {len(self.gpt_load_group_names)} ä¸ªç»„...")

            # éå†æ‰€æœ‰group namesï¼Œä¸ºæ¯ä¸ªgroupå‘é€keys
            all_success = True
            failed_groups = []
            
            for group_name in self.gpt_load_group_names:
                logger.info(f"ğŸ“ æ­£åœ¨å¤„ç†ç»„: {group_name}")
                
                # 1. è·å–group ID (ä½¿ç”¨ç¼“å­˜)
                group_id = self._get_gpt_load_group_id(group_name)
                
                if group_id is None:
                    logger.error(f"è·å– '{group_name}' çš„ç»„ ID å¤±è´¥")
                    failed_groups.append(group_name)
                    all_success = False
                    continue

                # 2. å‘é€keysåˆ°æŒ‡å®šgroup
                add_keys_url = f"{self.gpt_load_url}/api/keys/add-async"
                keys_text = ",".join(keys)
                
                add_headers = {
                    'Authorization': f'Bearer {self.gpt_load_auth}',
                    'Content-Type': 'application/json',
                    'User-Agent': 'HajimiKing/1.0'
                }

                payload = {
                    "group_id": group_id,
                    "keys_text": keys_text
                }

                logger.info(f"ğŸ“ æ­£åœ¨å°† {len(keys)} ä¸ªå¯†é’¥æ·»åŠ åˆ°ç»„ '{group_name}' (ID: {group_id})...")

                try:
                    # å‘é€æ·»åŠ keysè¯·æ±‚
                    add_response = requests.post(
                        add_keys_url,
                        headers=add_headers,
                        json=payload,
                        timeout=60
                    )

                    if add_response.status_code != 200:
                        logger.error(f"æ·»åŠ å¯†é’¥åˆ°ç»„ '{group_name}' å¤±è´¥: HTTP {add_response.status_code} - {add_response.text}")
                        failed_groups.append(group_name)
                        all_success = False
                        continue

                    # è§£ææ·»åŠ keyså“åº”
                    add_data = add_response.json()
                    
                    if add_data.get('code') != 0:
                        logger.error(f"æ·»åŠ å¯†é’¥ API å¯¹ç»„ '{group_name}' è¿”å›é”™è¯¯: {add_data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                        failed_groups.append(group_name)
                        all_success = False
                        continue

                    # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                    task_data = add_data.get('data', {})
                    task_type = task_data.get('task_type')
                    is_running = task_data.get('is_running')
                    total = task_data.get('total', 0)
                    response_group_name = task_data.get('group_name')

                    logger.info(f"âœ… ç»„ '{group_name}' çš„å¯†é’¥æ·»åŠ ä»»åŠ¡å·²æˆåŠŸå¯åŠ¨:")
                    logger.info(f"   ä»»åŠ¡ç±»å‹: {task_type}")
                    logger.info(f"   æ˜¯å¦è¿è¡Œ: {is_running}")
                    logger.info(f"   æ€»å¯†é’¥æ•°: {total}")
                    logger.info(f"   ç»„å: {response_group_name}")

                except Exception as e:
                    logger.error(f"âŒ å‘ç»„ '{group_name}' æ·»åŠ å¯†é’¥æ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
                    failed_groups.append(group_name)
                    all_success = False
                    continue

            # æ ¹æ®ç»“æœè¿”å›çŠ¶æ€
            if all_success:
                logger.info(f"âœ… å·²æˆåŠŸå°†å¯†é’¥å‘é€åˆ°æ‰€æœ‰ {len(self.gpt_load_group_names)} ä¸ªç»„")
                # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - æ‰€æœ‰å¯†é’¥éƒ½æˆåŠŸ
                send_result = {key: "ok" for key in keys}
                file_manager.save_keys_send_result(keys, send_result)
                return "ok"
            else:
                logger.error(f"âŒ å‘é€å¯†é’¥åˆ° {len(failed_groups)} ä¸ªç»„å¤±è´¥: {', '.join(failed_groups)}")
                # ä¿å­˜å‘é€ç»“æœæ—¥å¿— - éƒ¨åˆ†æˆ–å…¨éƒ¨å¤±è´¥
                send_result = {key: f"partial_failure_{len(failed_groups)}_groups" for key in keys}
                file_manager.save_keys_send_result(keys, send_result)
                return "partial_failure"

        except requests.exceptions.Timeout:
            logger.error("âŒ è¿æ¥ GPT load balancer æ—¶è¯·æ±‚è¶…æ—¶")
            send_result = {key: "timeout" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "timeout"
        except requests.exceptions.ConnectionError:
            logger.error("âŒ è¿æ¥ GPT load balancer å¤±è´¥")
            send_result = {key: "connection_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "connection_error"
        except json.JSONDecodeError as e:
            logger.error(f"âŒ GPT load balancer è¿”å›çš„ JSON æ— æ•ˆ: {str(e)}")
            send_result = {key: "json_decode_error" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "json_decode_error"
        except Exception as e:
            logger.error(f"âŒ å‘é€å¯†é’¥åˆ° GPT load balancer å¤±è´¥: {str(e)}", exc_info=True)
            send_result = {key: "exception" for key in keys}
            file_manager.save_keys_send_result(keys, send_result)
            return "exception"

    def _start_batch_sender(self) -> None:
        """å¯åŠ¨æ‰¹é‡å‘é€å®šæ—¶å™¨"""
        if self.shutdown_flag:
            return

        # å¯åŠ¨å‘é€ä»»åŠ¡
        self.executor.submit(self._batch_send_worker)

        # è®¾ç½®ä¸‹ä¸€æ¬¡å‘é€å®šæ—¶å™¨
        self.batch_timer = threading.Timer(self.batch_interval, self._start_batch_sender)
        self.batch_timer.daemon = True
        self.batch_timer.start()

    def _batch_send_worker(self) -> None:
        """æ‰¹é‡å‘é€worker"""
        while self.saving_checkpoint:
            logger.info(f"ğŸ“¥ [Sync] æ£€æŸ¥ç‚¹æ­£åœ¨ä¿å­˜ä¸­ï¼Œåœ¨æ‰¹é‡å‘é€å‰ç­‰å¾…...")
            time.sleep(0.5)

        self.saving_checkpoint = True
        try:
            # åŠ è½½checkpoint
            logger.info(f"ğŸ“¥ [Sync] å¼€å§‹æ‰¹é‡å‘é€ï¼Œwait_send_balancer é•¿åº¦: {len(checkpoint.wait_send_balancer)}, wait_send_gpt_load é•¿åº¦: {len(checkpoint.wait_send_gpt_load)}")
            # å‘é€gemini balanceré˜Ÿåˆ—
            if checkpoint.wait_send_balancer and self.balancer_enabled:
                balancer_keys = list(checkpoint.wait_send_balancer)
                logger.info(f"ğŸ”„ [Sync] æ­£åœ¨å¤„ç† gemini balancer é˜Ÿåˆ—ä¸­çš„ {len(balancer_keys)} ä¸ªå¯†é’¥")

                result_code = self._send_balancer_worker(balancer_keys)
                if result_code == 'ok':
                    # æ¸…ç©ºé˜Ÿåˆ—
                    checkpoint.wait_send_balancer.clear()
                    logger.info(f"âœ… Gemini balancer é˜Ÿåˆ—å¤„ç†æˆåŠŸï¼Œå·²æ¸…é™¤ {len(balancer_keys)} ä¸ªå¯†é’¥")
                else:
                    logger.error(f"âŒ Gemini balancer é˜Ÿåˆ—å¤„ç†å¤±è´¥ï¼Œé”™è¯¯ä»£ç : {result_code}")

            # å‘é€gpt_loadé˜Ÿåˆ—
            if checkpoint.wait_send_gpt_load and self.gpt_load_enabled:
                gpt_load_keys = list(checkpoint.wait_send_gpt_load)
                logger.info(f"ğŸ”„ æ­£åœ¨å¤„ç† GPT load balancer é˜Ÿåˆ—ä¸­çš„ {len(gpt_load_keys)} ä¸ªå¯†é’¥")

                result_code = self._send_gpt_load_worker(gpt_load_keys)

                if result_code == 'ok':
                    # æ¸…ç©ºé˜Ÿåˆ—
                    checkpoint.wait_send_gpt_load.clear()
                    logger.info(f"âœ… GPT load balancer é˜Ÿåˆ—å¤„ç†æˆåŠŸï¼Œå·²æ¸…é™¤ {len(gpt_load_keys)} ä¸ªå¯†é’¥")
                else:
                    logger.error(f"âŒ GPT load balancer é˜Ÿåˆ—å¤„ç†å¤±è´¥ï¼Œé”™è¯¯ä»£ç : {result_code}")

            # ä¿å­˜checkpoint
            file_manager.save_checkpoint(checkpoint)
        except Exception as e:
            stacktrace = traceback.format_exc()
            logger.error(f"âŒ æ‰¹é‡å‘é€å·¥ä½œå™¨é”™è¯¯: {e}\n{stacktrace}")
            logger.error(f"âŒ æ‰¹é‡å‘é€å·¥ä½œå™¨é”™è¯¯: {e}")
        finally:
            self.saving_checkpoint = False  # Release the lock

    def shutdown(self) -> None:
        """å…³é—­çº¿ç¨‹æ± å’Œå®šæ—¶å™¨"""
        self.shutdown_flag = True

        if self.batch_timer:
            self.batch_timer.cancel()

        self.executor.shutdown(wait=True)
        logger.info("ğŸ”š SyncUtils å…³é—­å®Œæˆ")


# åˆ›å»ºå…¨å±€å®ä¾‹
sync_utils = SyncUtils()
